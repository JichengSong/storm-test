<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

  <property>
    <name>mapred.tasktracker.map.tasks.maximum</name>
    <value>11</value>
  </property>
  <property>
    <name>mapred.tasktracker.reduce.tasks.maximum</name>
    <value>4</value>
  </property>

  <property>
    <name>mapred.task.timeout</name>
    <value>600000</value>
  </property>
  <property>
    <name>mapreduce.jobtracker.staging.root.dir</name>
    <value>/user</value>
  </property>
  <property>
    <name>mapred.jobtracker.taskScheduler.maxRunningTasksPerJob</name>
    <value>500</value>
  </property>
  <property>
    <name>mapred.reduce.slowstart.completed.maps</name>
    <value>0.75</value>
  </property>
  <property>
    <name>mapred.jobtracker.completeuserjobs.maximum</name>
    <value>16</value>
  </property>
  <property>
    <name>mapred.min.split.size</name>
    <value>0</value>
    <description>The minimum size chunk that map input should be split
    into.  Note that some file formats may have minimum split sizes that 
    take priority over this setting.</description>
  </property>
  <property>
    <name>mapred.jobtracker.maxtasks.per.job</name>
    <value>30000</value>
    <description>The maximum number of tasks for a single job. 
    A value of -1 indicates that there is no maximum.  </description>
  </property>

  <!--
  <property>
    <name>mapred.jobtracker.taskScheduler</name>
    <value>org.apache.hadoop.mapred.CapacityTaskScheduler</value>
  </property>
  -->
  <property>
  	<name>mapred.jobtracker.taskScheduler</name>
  	<value>org.apache.hadoop.mapred.FairScheduler</value>
  </property>
  <property>
  	<name>mapred.fairscheduler.allocation.file</name>
  	<value>/home/hadoop/hadoop/conf/fair-scheduler.xml</value>
  </property>
  <property>
  	<name>mapred.fairscheduler.assignmultiple</name>
  	<value>false</value>
  </property>
  <property>
  	<name>mapred.fairscheduler.sizebasedweight</name>
  	<value>true</value>
  </property>
  <property>
  	<name>mapred.fairscheduler.preemption</name>
  	<value>true</value>
  </property>
  <property>
  	<name>mapred.fairscheduler.eventlog.enabled</name>
  	<value>true</value>
  </property>

  <property> 
    <name>mapred.job.tracker.handler.count</name> 
    <value>50</value> 
    <description> 
    The number of server threads for the JobTracker. This should be roughly 
    4% of the number of tasktracker nodes. 
    </description> 
  </property>
  <property>
    <name>mapred.job.tracker</name>
    <value>YZSJHL18-22.opi.com:8021</value>
  </property>
  <property>
    <name>mapred.child.java.opts</name>
    <value>-Xmx2500m -Xss256k -XX:+UseConcMarkSweepGC -verbose:gc -Xloggc:/tmp/@taskid@.gc</value>
  </property>
  <!--
  <property>
    <name>mapred.compress.map.output</name>
    <value>true</value>
  </property>
  
  <property>
    <name>mapred.map.output.compression.codec</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
  </property>
  -->

  <property>
    <name>mapred.local.dir</name>
    <value>/data1/hadoop/mr.local,/data2/hadoop/mr.local,/data3/hadoop/mr.local,/data4/hadoop/mr.local,/data5/hadoop/mr.local,/data6/hadoop/mr.local,/data7/hadoop/mr.local,/data8/hadoop/mr.local,/data9/hadoop/mr.local,/data10/hadoop/mr.local,/data11/hadoop/mr.local,/data12/hadoop/mr.local</value>
  </property>

<!-- begin for renren snapshot -->

<property>
  <name>jobtracker.servers</name>
  <!--<value>YZSJHL18-22.opi.com:8021,YZSJHL18-24.opi.com:8021</value>-->
  <value>YZSJHL18-22.opi.com:8021</value>
  <description>job tracker server addresses for HA</description>
</property>
<property>
  <name>jobtracker.zookeeper.quorum</name>
  <value>10.4.18.25:2222,10.4.18.22:2222,10.4.18.23:2222,10.4.18.24:2222,10.4.18.32:2222,10.4.18.33:2222,10.4.18.34:2222</value>
  <description>job tracker connect zookeeper address</description>
</property>
<property>
  <name>jobtracker.zookeeper.timeout</name>
  <value>10</value>
  <description>job tracker connect zookeeper timeout,unit is second</description>
</property>
<property>
  <name>mapred.tasktracker.cgroup</name>
  <value>-g memory:hadoop-node/default</value>
</property>
<property>
  <name>mapred.job.tracker.persist.jobstatus.hours</name>
  <value>1</value>
  <description>The number of hours job status information is persisted in DFS.
    The job status information will be available after it drops of the memory
    queue and between jobtracker restarts. With a zero value the job status
    information is not persisted at all in DFS.
  </description>
</property>
<property>
    <name>mapred.job.tracker.persist.jobstatus.active</name>
    <value>true</value>
    <description>Indicates if persistency of job status information is active or not.  </description>
</property>
<property>
  <name>mapred.jobtracker.restart.recover</name>
  <value>true</value>
  <description>"true" to enable (job) recovery upon restart, "false" to start afresh </description>
</property>

<property>
  <name>hadoop.job.history.location</name>
  <value>/jobtracker/job.history</value>
  <description> If job tracker is static the history files are stored
  in this single well known place. If No value is set here, by default,
  it is in the local file system at ${hadoop.log.dir}/history.
  </description>
</property>
<property> 
  <name>mapred.jobtracker.job.history.block.size</name> 
  <value>3072000</value> 
  <description>The block size of the job history file. Since the job recovery 
  uses job history, its important to dump job history to disk as  
  soon as possible. Note that this is an expert level parameter. 
  The default value is set to 3 MB. 
  </description> 
</property>
<property>
  <name>mapred.system.dir</name>
  <value>/jobtracker/system</value>
  <description>The directory where MapReduce stores control files.  </description>
</property>
<property>
      <name>mapreduce.admin.map.child.java.opts</name>
      <value>-Xmx2500m</value>
</property>
<property>
      <name>mapreduce.admin.reduce.child.java.opts</name>
      <value>-Xmx2500m</value>
</property>
<!-- end for renren snapshot -->
</configuration>
